# Функции потерь в машинном обучении

Функции потерь (loss functions) используются для оценки качества предсказаний модели. Ниже приведены пять популярных функций потерь, часто встречающихся в задачах классификации (особенно в контексте SVM, логистической регрессии, AdaBoost и т.д.). Для каждой указана математическая запись, тип, свойства и области применения.

---

## 1. **Hinge Loss** (Шарнирная функция потерь) — для SVM
$$
\mathcal{L}(w, x_i, y_i) = \max(0, 1 - w^T x_i y_i)
$$

- **Тип**: кусочно-линейная  
- **Производная**:  
  $$
  \frac{\partial \mathcal{L}}{\partial w} = 
  \begin{cases} 
  0, & w^T x_i y_i \geq 1 \\
  -y_i x_i, & w^T x_i y_i < 1 
  \end{cases}
  $$
- **Особенности**:  
  - Наказывает только за ошибки классификации и малую уверенность (margin < 1).  
  - Устойчива к выбросам.  
- **Применение**: Support Vector Machines (SVM), задачи с разделяющей гиперплоскостью.

---

## 2. **Log-loss (Binary Cross-Entropy / Logistic Loss)**  
$$
\mathcal{L}(w, x_i, y_i) = \log_2 (1 + e^{-w^T x_i y_i})
$$

- **Тип**: логарифмическая, выпуклая, гладкая  
- **Эквивалентные формы**:  
  $$
  \mathcal{L} = \log(1 + e^{-z}), \quad z = w^T x_i y_i
  $$
  (в базе \(e\))  
- **Особенности**:  
  - Растёт медленно при больших ошибках → устойчива к шуму.  
  - Даёт вероятностную интерпретацию.  
- **Применение**: логистическая регрессия, нейронные сети, бинарная классификация.

---

## 3. **Squared Hinge Loss** (Квадратичная шарнирная потеря)  
$$
\mathcal{L}(w, x_i, y_i) = \max(0, 1 - w^T x_i y_i)^2
$$

- **Тип**: квадратичная  
- **Особенности**:  
  - Сильнее наказывает за нарушения маргина, чем Hinge.  
  - Гладкая в области ошибки → лучше для градиентных методов.  
- **Применение**: SVM с квадратичным штрафом, задачи с высокой чувствительностью к margin.

---

## 4. **Sigmoid (Cross-Entropy) Loss** — на самом деле это **Logistic Loss в другой форме**  
$$
\mathcal{L}(w, x_i, y_i) = 2 \cdot (1 + e^{w^T x_i y_i})^{-1}
$$

> Упростим:  
$$
\mathcal{L} = \frac{2}{1 + e^{w^T x_i y_i}} = \frac{2}{1 + e^{z}}
$$

Но это **не стандартная сигмоидальная потеря**.  
**Правильная форма Logistic Loss**:  
$$
\mathcal{L}(w, x_i, y_i) = \log(1 + e^{-w^T x_i y_i})
$$

А **бинарная кросс-энтропия** для вероятностей:  
$$
\mathcal{L}(p, y) = -y \log p - (1-y) \log(1-p), \quad p = \sigma(w^T x)
$$

- **Оригинальная формула** эквивалентна **масштабированной сигмоиде**, но **не является стандартной функцией потерь**.  
- **Применение**: редко в чистом виде; чаще — как часть кросс-энтропии.

---

## 5. **Exponential Loss** — используется в AdaBoost  
$$
\mathcal{L}(w, x_i, y_i) = e^{-w^T x_i y_i}
$$

- **Тип**: экспоненциальная, выпуклая  
- **Особенности**:  
  - Очень сильно наказывает за ошибки (экспоненциальный рост).  
  - Чувствительна к выбросам.  
  - Оптимизация минимизирует верхнюю границу ошибки классификации.  
- **Применение**: AdaBoost, бустинг, ансамбли слабых классификаторов.

---

## Сравнение функций потерь

| Функция             | Рост при ошибке       | Гладкость       | Устойчивость к шуму | Применение                     |
|---------------------|------------------------|------------------|----------------------|--------------------------------|
| Hinge               | Линейный              | Нет             | Высокая              | SVM                            |
| Log-loss            | Логарифмический       | Да              | Высокая              | Логистическая регрессия        |
| Squared Hinge       | Квадратичный          | Да (в ошибке)   | Средняя              | SVM с гладкой оптимизацией     |
| Exponential         | Экспоненциальный      | Да              | Низкая               | AdaBoost                       |

---

## Вывод

Каждая функция потерь подходит для определённых задач:
- **Hinge** — для жёстких границ (SVM).
- **Log-loss** — для вероятностных моделей.
- **Exponential** — для бустинга.
- **Squared Hinge** — компромисс между гладкостью и строгостью.

> Выбор функции потерь влияет на **устойчивость, скорость сходимости и интерпретируемость** модели.